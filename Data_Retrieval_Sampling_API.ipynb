{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c8c9b2",
   "metadata": {},
   "source": [
    "**Data Retrieval, Sampling and Incivility scores (PerspectiveAPI)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9fe3cc",
   "metadata": {},
   "source": [
    "**Content**\n",
    "\n",
    "For each Subreddit:\n",
    "1. Functions for generating .csv from .zst (Reddit Dump format)\n",
    "\n",
    "2. Sampling 19 Comments for each month (from 2012 until 2024)\n",
    "\n",
    "3. Calling PerspectiveAPI to label each comment and saving to {subreddit}_with_scores.csv for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "import pandas as pd\n",
    "from googleapiclient import discovery # need to pip install too\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating csv out of Reddit dump / comments zst to csv\n",
    "# EXAMPLE file_path = \"liberal_comments.zst\"\n",
    "# EXAMPLE output_file = \"liberal_comments.csv\"\n",
    "def generate_csv_from_dump(file_path, output_file):\n",
    "\n",
    "\n",
    "    comment_fields = [\n",
    "        \"id\", \"parent_id\", \"link_id\", \"author\", \"author_flair_text\", \"body\",\n",
    "        \"created_utc\", \"score\", \"controversiality\", \"distinguished\", \"edited\",\n",
    "        \"gilded\", \"is_submitter\", \"stickied\", \"subreddit\", \"subreddit_id\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    lines_processed = 0\n",
    "    bad_lines = 0\n",
    "    buffer = \"\"\n",
    "\n",
    "    with open(file_path, \"rb\") as f_in, open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        reader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(f_in)\n",
    "        writer = csv.DictWriter(f_out, fieldnames=comment_fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "        while True:\n",
    "            chunk = reader.read(2**25)\n",
    "            if not chunk:\n",
    "                break\n",
    "            chunk = chunk.decode(errors='replace')\n",
    "            lines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    row = {k: obj.get(k, \"\") for k in comment_fields}  # fehlende Keys als \"\"\n",
    "                    writer.writerow(row)\n",
    "                    lines_processed += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    bad_lines += 1\n",
    "\n",
    "            buffer = lines[-1]\n",
    "\n",
    "    print(f\"Fertig! {lines_processed} Zeilen verarbeitet, {bad_lines} fehlerhafte JSON-Zeilen.\")\n",
    "    print(f\"Daten gespeichert in: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74454599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First preprocessing step: take 100 comments per year.\n",
    "# oldest comment: 2009-05-03, newest comment: 2024-12-31\n",
    "# we can start from 2012 - complete until 2024\n",
    "\n",
    "\n",
    "\n",
    "def Extract_Monthly_samples(df, Subreddit):\n",
    "    data = df.copy()\n",
    "    data[\"year\"] = pd.to_datetime(data[\"created_utc\"], unit=\"s\").dt.year\n",
    "    data[\"month\"] = pd.to_datetime(data[\"created_utc\"], unit=\"s\").dt.month\n",
    "\n",
    "    # no empty or deleted samples\n",
    "    mask = (\n",
    "        data[\"body\"].notna()\n",
    "        & ~data[\"body\"].isin([\"[deleted]\", \"[removed]\", \"\"])\n",
    "    )\n",
    "    data = data[mask]\n",
    "\n",
    "    # Filter years\n",
    "    sampled = data[(data[\"year\"] > 2011) & (data[\"year\"] < 2025)]# Sample 19 per (year, month)\n",
    "    \n",
    "    sampled_csv = (\n",
    "        sampled\n",
    "        .groupby([\"year\", \"month\"], group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=min(len(g), 19), random_state=42))\n",
    "    )\n",
    "    sampled_csv.to_csv(f\"{Subreddit}_sampled.csv\", index=False)\n",
    "    \n",
    "    return sampled_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd387bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "API_KEY = ''\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def Generate_Incivility_Ratings(data, Subreddit):\n",
    "# initialise new column\n",
    "    data['incivility_score'] = 0.0 \n",
    "\n",
    "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        comment = row['body']\n",
    "\n",
    "       # checking for empty comments\n",
    "        if not comment or comment.strip() == \"\":\n",
    "            continue\n",
    "        if row['incivility_score'] != 0: \n",
    "            continue\n",
    "\n",
    "        analyze_request = {\n",
    "            'comment': {'text': comment},\n",
    "            'requestedAttributes': {'TOXICITY': {}},\n",
    "            'languages': ['en']   \n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "            data.at[idx, 'incivility_score'] = response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {idx}: {e}\")\n",
    "\n",
    "        if idx % 250 == 0 and idx > 0:\n",
    "            data.to_csv(f\"{Subreddit}_with_scores.csv\", index=False)\n",
    "            print(f\"Zwischenspeicherung bei Index {idx}\")\n",
    "\n",
    "        time.sleep(.8) # rate is 1 request per second\n",
    "\n",
    "    data.to_csv(f\"{Subreddit}_with_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65641d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig! 497079 Zeilen verarbeitet, 0 fehlerhafte JSON-Zeilen.\n",
      "Daten gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\Liberal_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Starting with Liberal\n",
    "file_path = \"Liberal_comments.zst\"\n",
    "output_file = \"Liberal_comments.csv\"\n",
    "\n",
    "generate_csv_from_dump(file_path, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c0655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TG2023\\AppData\\Local\\Temp\\ipykernel_7456\\4254740137.py:2: DtypeWarning: Columns (4,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(output_file)\n",
      "C:\\Users\\TG2023\\AppData\\Local\\Temp\\ipykernel_7456\\1183392153.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=min(len(g), 19), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "output_file = \"Liberal_comments.csv\"\n",
    "df = pd.read_csv(output_file)\n",
    "Subreddit = \"Liberal\"\n",
    "samples_Liberal = Extract_Monthly_samples(df, Subreddit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eebe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:47<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# testing with smaller sample size \n",
    "test = samples_Liberal.sample(n=40, random_state=42) \n",
    "Generate_Incivility_Ratings(test, Subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c77386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2964/2964 [00:00<00:00, 24093.10it/s]\n"
     ]
    }
   ],
   "source": [
    "samples_Liberal = pd.read_csv(r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\Liberal_with_scores.csv\")\n",
    "Subreddit = \"Liberal\"\n",
    "Generate_Incivility_Ratings(samples_Liberal, Subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83190a6",
   "metadata": {},
   "source": [
    "2. Import And Rate Conservative Subreddit Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b5d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig! 18984143 Zeilen verarbeitet, 0 fehlerhafte JSON-Zeilen.\n",
      "Daten gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\Conservative_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"Conservative_comments.zst\"\n",
    "# output_file = \"Conservative_comments.csv\"\n",
    "\n",
    "# generate_csv_from_dump(file_path, output_file)\n",
    "# # took 8 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\Conservative_comments.csv\"\n",
    "df = pd.read_csv(output_file)\n",
    "Subreddit = \"Conservative\"\n",
    "samples_Conservative = Extract_Monthly_samples(df, Subreddit)\n",
    "# took 8 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a66827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:48<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# # testing with smaller sample size \n",
    "# test = samples_Conservative.sample(n=40, random_state=42) \n",
    "# Generate_Incivility_Ratings(test, Subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b202bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test worked\n",
    "# test_csv = pd.read_csv(r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\Conservative_with_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e977799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test worked\n",
    "samples_Conservative = pd.read_csv(r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\Conservative_with_scores.csv\")\n",
    "Subreddit = \"Conservative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5099e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2964/2964 [08:44<00:00,  5.65it/s]\n"
     ]
    }
   ],
   "source": [
    "Generate_Incivility_Ratings(samples_Conservative, Subreddit)\n",
    "# done after 5h 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8823b55",
   "metadata": {},
   "source": [
    "3. Import and Rating of r/funny posts n = 3420 (originally, before settling on 2012 -2024 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fff7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig! 117525270 Zeilen verarbeitet, 0 fehlerhafte JSON-Zeilen.\n",
      "Daten gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\funny_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# r /funny\n",
    "file_path = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\funny_comments.zst\"\n",
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\funny_comments.csv\"\n",
    "\n",
    "generate_csv_from_dump(file_path, output_file)\n",
    "\n",
    "# took 35 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ec6884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: total samples so far = 0\n",
      "Chunk 2: total samples so far = 0\n",
      "Chunk 3: total samples so far = 0\n",
      "Chunk 4: total samples so far = 0\n",
      "Chunk 5: total samples so far = 0\n",
      "Chunk 6: total samples so far = 0\n",
      "Chunk 7: total samples so far = 0\n",
      "Chunk 8: total samples so far = 38\n",
      "Chunk 9: total samples so far = 57\n",
      "Chunk 10: total samples so far = 76\n",
      "Chunk 11: total samples so far = 95\n",
      "Chunk 12: total samples so far = 114\n",
      "Chunk 13: total samples so far = 133\n",
      "Chunk 14: total samples so far = 133\n",
      "Chunk 15: total samples so far = 152\n",
      "Chunk 16: total samples so far = 171\n",
      "Chunk 17: total samples so far = 190\n",
      "Chunk 18: total samples so far = 209\n",
      "Chunk 19: total samples so far = 228\n",
      "Chunk 20: total samples so far = 228\n",
      "Chunk 21: total samples so far = 247\n",
      "Chunk 22: total samples so far = 266\n",
      "Chunk 23: total samples so far = 285\n",
      "Chunk 24: total samples so far = 285\n",
      "Chunk 25: total samples so far = 304\n",
      "Chunk 26: total samples so far = 323\n",
      "Chunk 27: total samples so far = 342\n",
      "Chunk 28: total samples so far = 361\n",
      "Chunk 29: total samples so far = 380\n",
      "Chunk 30: total samples so far = 380\n",
      "Chunk 31: total samples so far = 399\n",
      "Chunk 32: total samples so far = 418\n",
      "Chunk 33: total samples so far = 437\n",
      "Chunk 34: total samples so far = 456\n",
      "Chunk 35: total samples so far = 475\n",
      "Chunk 36: total samples so far = 494\n",
      "Chunk 37: total samples so far = 513\n",
      "Chunk 38: total samples so far = 532\n",
      "Chunk 39: total samples so far = 551\n",
      "Chunk 40: total samples so far = 570\n",
      "Chunk 41: total samples so far = 589\n",
      "Chunk 42: total samples so far = 608\n",
      "Chunk 43: total samples so far = 627\n",
      "Chunk 44: total samples so far = 646\n",
      "Chunk 45: total samples so far = 665\n",
      "Chunk 46: total samples so far = 703\n",
      "Chunk 47: total samples so far = 722\n",
      "Chunk 48: total samples so far = 741\n",
      "Chunk 49: total samples so far = 760\n",
      "Chunk 50: total samples so far = 798\n",
      "Chunk 51: total samples so far = 817\n",
      "Chunk 52: total samples so far = 836\n",
      "Chunk 53: total samples so far = 874\n",
      "Chunk 54: total samples so far = 893\n",
      "Chunk 55: total samples so far = 931\n",
      "Chunk 56: total samples so far = 950\n",
      "Chunk 57: total samples so far = 988\n",
      "Chunk 58: total samples so far = 1007\n",
      "Chunk 59: total samples so far = 1045\n",
      "Chunk 60: total samples so far = 1083\n",
      "Chunk 61: total samples so far = 1121\n",
      "Chunk 62: total samples so far = 1140\n",
      "Chunk 63: total samples so far = 1178\n",
      "Chunk 64: total samples so far = 1216\n",
      "Chunk 65: total samples so far = 1235\n",
      "Chunk 66: total samples so far = 1273\n",
      "Chunk 67: total samples so far = 1311\n",
      "Chunk 68: total samples so far = 1349\n",
      "Chunk 69: total samples so far = 1387\n",
      "Chunk 70: total samples so far = 1406\n",
      "Chunk 71: total samples so far = 1444\n",
      "Chunk 72: total samples so far = 1463\n",
      "Chunk 73: total samples so far = 1482\n",
      "Chunk 74: total samples so far = 1501\n",
      "Chunk 75: total samples so far = 1539\n",
      "Chunk 76: total samples so far = 1558\n",
      "Chunk 77: total samples so far = 1577\n",
      "Chunk 78: total samples so far = 1596\n",
      "Chunk 79: total samples so far = 1615\n",
      "Chunk 80: total samples so far = 1615\n",
      "Chunk 81: total samples so far = 1634\n",
      "Chunk 82: total samples so far = 1653\n",
      "Chunk 83: total samples so far = 1672\n",
      "Chunk 84: total samples so far = 1691\n",
      "Chunk 85: total samples so far = 1710\n",
      "Chunk 86: total samples so far = 1729\n",
      "Chunk 87: total samples so far = 1767\n",
      "Chunk 88: total samples so far = 1786\n",
      "Chunk 89: total samples so far = 1805\n",
      "Chunk 90: total samples so far = 1843\n",
      "Chunk 91: total samples so far = 1862\n",
      "Chunk 92: total samples so far = 1881\n",
      "Chunk 93: total samples so far = 1900\n",
      "Chunk 94: total samples so far = 1938\n",
      "Chunk 95: total samples so far = 1957\n",
      "Chunk 96: total samples so far = 1976\n",
      "Chunk 97: total samples so far = 2014\n",
      "Chunk 98: total samples so far = 2033\n",
      "Chunk 99: total samples so far = 2071\n",
      "Chunk 100: total samples so far = 2090\n",
      "Chunk 101: total samples so far = 2128\n",
      "Chunk 102: total samples so far = 2166\n",
      "Chunk 103: total samples so far = 2204\n",
      "Chunk 104: total samples so far = 2242\n",
      "Chunk 105: total samples so far = 2280\n",
      "Chunk 106: total samples so far = 2318\n",
      "Chunk 107: total samples so far = 2356\n",
      "Chunk 108: total samples so far = 2394\n",
      "Chunk 109: total samples so far = 2432\n",
      "Chunk 110: total samples so far = 2470\n",
      "Chunk 111: total samples so far = 2508\n",
      "Chunk 112: total samples so far = 2546\n",
      "Chunk 113: total samples so far = 2584\n",
      "Chunk 114: total samples so far = 2660\n",
      "Chunk 115: total samples so far = 2755\n",
      "Chunk 116: total samples so far = 2831\n",
      "Chunk 117: total samples so far = 2926\n",
      "Chunk 118: total samples so far = 2964\n",
      "Fertig! Samples gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\funny_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameter\n",
    "input_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\funny_comments.csv\"\n",
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\funny_sampled.csv\"\n",
    "target_subreddit = \"funny\"\n",
    "samples_per_month = 19\n",
    "chunksize = 10**6\n",
    "\n",
    "cols = [\"id\", \"parent_id\", \"link_id\", \"author\", \"body\", \"created_utc\", \"score\", \"subreddit\"]\n",
    "dtypes = {\n",
    "    \"id\": \"string\", \"parent_id\": \"string\", \"link_id\": \"string\",\n",
    "    \"author\": \"string\", \"body\": \"string\", \"created_utc\": \"int64\",\n",
    "    \"score\": \"int32\", \"subreddit\": \"string\"\n",
    "}\n",
    "\n",
    "reservoirs = defaultdict(list)\n",
    "counts = defaultdict(int)\n",
    "start_year, end_year = 2012, 2024\n",
    "\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(input_file, usecols=cols, dtype=dtypes, chunksize=chunksize):\n",
    "    chunk_num += 1\n",
    "    subset = chunk[chunk[\"subreddit\"] == target_subreddit]\n",
    "    created_dt = pd.to_datetime(subset[\"created_utc\"], unit='s')\n",
    "    months = created_dt.dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    for row, month in zip(subset.itertuples(index=False), months):\n",
    "        if pd.isna(row.body) or row.body in (\"[deleted]\", \"[removed]\", \"\"):\n",
    "            continue\n",
    "        year = int(month[:4])\n",
    "        if start_year <= year <= end_year:\n",
    "            counts[month] += 1\n",
    "            idx = counts[month]\n",
    "            if len(reservoirs[month]) < samples_per_month:\n",
    "                reservoirs[month].append(row)\n",
    "            else:\n",
    "                j = random.randint(0, idx - 1)\n",
    "                if j < samples_per_month:\n",
    "                    reservoirs[month][j] = row\n",
    "\n",
    "    if chunk_num % 1 == 0:\n",
    "        total_samples = sum(len(v) for v in reservoirs.values())\n",
    "        print(f\"Chunk {chunk_num}: total samples so far = {total_samples}\")\n",
    "\n",
    "# Flatten reservoirs\n",
    "final_samples = []\n",
    "for month, rows in sorted(reservoirs.items()):\n",
    "    for row in rows:\n",
    "        r = row._asdict()\n",
    "        r[\"month\"] = month\n",
    "        final_samples.append(r)\n",
    "\n",
    "df_samples = pd.DataFrame(final_samples)\n",
    "df_samples.to_csv(output_file, index=False)\n",
    "print(\"Fertig! Samples gespeichert in:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c8d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff0dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing with smaller sample size \n",
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\funny_with_scores.csv\"\n",
    "Subreddit = \"funny\"\n",
    "samples_funny = pd.read_csv(output_file)\n",
    "# test = samples_funny.sample(n=40, random_state=42) \n",
    "# Generate_Incivility_Ratings(test, Subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a622a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1897/2964 [00:08<00:11, 95.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwischenspeicherung bei Index 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2964/2964 [00:16<00:00, 175.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Generate_Incivility_Ratings(samples_funny, Subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826f668",
   "metadata": {},
   "source": [
    "4. Import and rating of r/ politics n = 3420\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig! 201460965 Zeilen verarbeitet, 0 fehlerhafte JSON-Zeilen.\n",
      "Daten gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\politics_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# # r/ politics\n",
    "# file_path = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\politics_comments.zst\"\n",
    "# output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\politics_comments.csv\"\n",
    "\n",
    "# generate_csv_from_dump(file_path, output_file)\n",
    "\n",
    "# # took 94 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15038496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: total samples so far = 0\n",
      "Chunk 2: total samples so far = 0\n",
      "Chunk 3: total samples so far = 0\n",
      "Chunk 4: total samples so far = 0\n",
      "Chunk 5: total samples so far = 0\n",
      "Chunk 6: total samples so far = 0\n",
      "Chunk 7: total samples so far = 0\n",
      "Chunk 8: total samples so far = 0\n",
      "Chunk 9: total samples so far = 19\n",
      "Chunk 10: total samples so far = 57\n",
      "Chunk 11: total samples so far = 114\n",
      "Chunk 12: total samples so far = 152\n",
      "Chunk 13: total samples so far = 190\n",
      "Chunk 14: total samples so far = 209\n",
      "Chunk 15: total samples so far = 247\n",
      "Chunk 16: total samples so far = 304\n",
      "Chunk 17: total samples so far = 342\n",
      "Chunk 18: total samples so far = 399\n",
      "Chunk 19: total samples so far = 456\n",
      "Chunk 20: total samples so far = 532\n",
      "Chunk 21: total samples so far = 627\n",
      "Chunk 22: total samples so far = 703\n",
      "Chunk 23: total samples so far = 798\n",
      "Chunk 24: total samples so far = 855\n",
      "Chunk 25: total samples so far = 893\n",
      "Chunk 26: total samples so far = 912\n",
      "Chunk 27: total samples so far = 950\n",
      "Chunk 28: total samples so far = 950\n",
      "Chunk 29: total samples so far = 969\n",
      "Chunk 30: total samples so far = 988\n",
      "Chunk 31: total samples so far = 988\n",
      "Chunk 32: total samples so far = 1007\n",
      "Chunk 33: total samples so far = 1026\n",
      "Chunk 34: total samples so far = 1045\n",
      "Chunk 35: total samples so far = 1045\n",
      "Chunk 36: total samples so far = 1064\n",
      "Chunk 37: total samples so far = 1064\n",
      "Chunk 38: total samples so far = 1083\n",
      "Chunk 39: total samples so far = 1102\n",
      "Chunk 40: total samples so far = 1102\n",
      "Chunk 41: total samples so far = 1102\n",
      "Chunk 42: total samples so far = 1121\n",
      "Chunk 43: total samples so far = 1121\n",
      "Chunk 44: total samples so far = 1140\n",
      "Chunk 45: total samples so far = 1140\n",
      "Chunk 46: total samples so far = 1159\n",
      "Chunk 47: total samples so far = 1159\n",
      "Chunk 48: total samples so far = 1178\n",
      "Chunk 49: total samples so far = 1178\n",
      "Chunk 50: total samples so far = 1197\n",
      "Chunk 51: total samples so far = 1197\n",
      "Chunk 52: total samples so far = 1216\n",
      "Chunk 53: total samples so far = 1235\n",
      "Chunk 54: total samples so far = 1235\n",
      "Chunk 55: total samples so far = 1235\n",
      "Chunk 56: total samples so far = 1254\n",
      "Chunk 57: total samples so far = 1273\n",
      "Chunk 58: total samples so far = 1273\n",
      "Chunk 59: total samples so far = 1292\n",
      "Chunk 60: total samples so far = 1292\n",
      "Chunk 61: total samples so far = 1311\n",
      "Chunk 62: total samples so far = 1330\n",
      "Chunk 63: total samples so far = 1330\n",
      "Chunk 64: total samples so far = 1349\n",
      "Chunk 65: total samples so far = 1368\n",
      "Chunk 66: total samples so far = 1368\n",
      "Chunk 67: total samples so far = 1387\n",
      "Chunk 68: total samples so far = 1387\n",
      "Chunk 69: total samples so far = 1406\n",
      "Chunk 70: total samples so far = 1406\n",
      "Chunk 71: total samples so far = 1425\n",
      "Chunk 72: total samples so far = 1425\n",
      "Chunk 73: total samples so far = 1444\n",
      "Chunk 74: total samples so far = 1444\n",
      "Chunk 75: total samples so far = 1463\n",
      "Chunk 76: total samples so far = 1482\n",
      "Chunk 77: total samples so far = 1482\n",
      "Chunk 78: total samples so far = 1501\n",
      "Chunk 79: total samples so far = 1501\n",
      "Chunk 80: total samples so far = 1520\n",
      "Chunk 81: total samples so far = 1520\n",
      "Chunk 82: total samples so far = 1539\n",
      "Chunk 83: total samples so far = 1539\n",
      "Chunk 84: total samples so far = 1539\n",
      "Chunk 85: total samples so far = 1558\n",
      "Chunk 86: total samples so far = 1558\n",
      "Chunk 87: total samples so far = 1577\n",
      "Chunk 88: total samples so far = 1577\n",
      "Chunk 89: total samples so far = 1596\n",
      "Chunk 90: total samples so far = 1596\n",
      "Chunk 91: total samples so far = 1615\n",
      "Chunk 92: total samples so far = 1615\n",
      "Chunk 93: total samples so far = 1615\n",
      "Chunk 94: total samples so far = 1634\n",
      "Chunk 95: total samples so far = 1634\n",
      "Chunk 96: total samples so far = 1653\n",
      "Chunk 97: total samples so far = 1653\n",
      "Chunk 98: total samples so far = 1672\n",
      "Chunk 99: total samples so far = 1672\n",
      "Chunk 100: total samples so far = 1691\n",
      "Chunk 101: total samples so far = 1710\n",
      "Chunk 102: total samples so far = 1710\n",
      "Chunk 103: total samples so far = 1729\n",
      "Chunk 104: total samples so far = 1729\n",
      "Chunk 105: total samples so far = 1748\n",
      "Chunk 106: total samples so far = 1748\n",
      "Chunk 107: total samples so far = 1767\n",
      "Chunk 108: total samples so far = 1767\n",
      "Chunk 109: total samples so far = 1786\n",
      "Chunk 110: total samples so far = 1786\n",
      "Chunk 111: total samples so far = 1805\n",
      "Chunk 112: total samples so far = 1805\n",
      "Chunk 113: total samples so far = 1824\n",
      "Chunk 114: total samples so far = 1824\n",
      "Chunk 115: total samples so far = 1824\n",
      "Chunk 116: total samples so far = 1843\n",
      "Chunk 117: total samples so far = 1843\n",
      "Chunk 118: total samples so far = 1862\n",
      "Chunk 119: total samples so far = 1862\n",
      "Chunk 120: total samples so far = 1862\n",
      "Chunk 121: total samples so far = 1881\n",
      "Chunk 122: total samples so far = 1881\n",
      "Chunk 123: total samples so far = 1881\n",
      "Chunk 124: total samples so far = 1900\n",
      "Chunk 125: total samples so far = 1900\n",
      "Chunk 126: total samples so far = 1900\n",
      "Chunk 127: total samples so far = 1919\n",
      "Chunk 128: total samples so far = 1919\n",
      "Chunk 129: total samples so far = 1938\n",
      "Chunk 130: total samples so far = 1938\n",
      "Chunk 131: total samples so far = 1957\n",
      "Chunk 132: total samples so far = 1957\n",
      "Chunk 133: total samples so far = 1976\n",
      "Chunk 134: total samples so far = 1976\n",
      "Chunk 135: total samples so far = 1995\n",
      "Chunk 136: total samples so far = 1995\n",
      "Chunk 137: total samples so far = 1995\n",
      "Chunk 138: total samples so far = 2014\n",
      "Chunk 139: total samples so far = 2014\n",
      "Chunk 140: total samples so far = 2014\n",
      "Chunk 141: total samples so far = 2033\n",
      "Chunk 142: total samples so far = 2033\n",
      "Chunk 143: total samples so far = 2033\n",
      "Chunk 144: total samples so far = 2033\n",
      "Chunk 145: total samples so far = 2033\n",
      "Chunk 146: total samples so far = 2033\n",
      "Chunk 147: total samples so far = 2052\n",
      "Chunk 148: total samples so far = 2052\n",
      "Chunk 149: total samples so far = 2071\n",
      "Chunk 150: total samples so far = 2071\n",
      "Chunk 151: total samples so far = 2071\n",
      "Chunk 152: total samples so far = 2071\n",
      "Chunk 153: total samples so far = 2090\n",
      "Chunk 154: total samples so far = 2090\n",
      "Chunk 155: total samples so far = 2109\n",
      "Chunk 156: total samples so far = 2109\n",
      "Chunk 157: total samples so far = 2128\n",
      "Chunk 158: total samples so far = 2147\n",
      "Chunk 159: total samples so far = 2166\n",
      "Chunk 160: total samples so far = 2185\n",
      "Chunk 161: total samples so far = 2185\n",
      "Chunk 162: total samples so far = 2204\n",
      "Chunk 163: total samples so far = 2223\n",
      "Chunk 164: total samples so far = 2242\n",
      "Chunk 165: total samples so far = 2261\n",
      "Chunk 166: total samples so far = 2280\n",
      "Chunk 167: total samples so far = 2299\n",
      "Chunk 168: total samples so far = 2337\n",
      "Chunk 169: total samples so far = 2356\n",
      "Chunk 170: total samples so far = 2375\n",
      "Chunk 171: total samples so far = 2394\n",
      "Chunk 172: total samples so far = 2394\n",
      "Chunk 173: total samples so far = 2413\n",
      "Chunk 174: total samples so far = 2432\n",
      "Chunk 175: total samples so far = 2451\n",
      "Chunk 176: total samples so far = 2470\n",
      "Chunk 177: total samples so far = 2489\n",
      "Chunk 178: total samples so far = 2508\n",
      "Chunk 179: total samples so far = 2527\n",
      "Chunk 180: total samples so far = 2546\n",
      "Chunk 181: total samples so far = 2584\n",
      "Chunk 182: total samples so far = 2603\n",
      "Chunk 183: total samples so far = 2622\n",
      "Chunk 184: total samples so far = 2641\n",
      "Chunk 185: total samples so far = 2660\n",
      "Chunk 186: total samples so far = 2698\n",
      "Chunk 187: total samples so far = 2717\n",
      "Chunk 188: total samples so far = 2755\n",
      "Chunk 189: total samples so far = 2774\n",
      "Chunk 190: total samples so far = 2793\n",
      "Chunk 191: total samples so far = 2831\n",
      "Chunk 192: total samples so far = 2850\n",
      "Chunk 193: total samples so far = 2869\n",
      "Chunk 194: total samples so far = 2869\n",
      "Chunk 195: total samples so far = 2888\n",
      "Chunk 196: total samples so far = 2907\n",
      "Chunk 197: total samples so far = 2907\n",
      "Chunk 198: total samples so far = 2926\n",
      "Chunk 199: total samples so far = 2945\n",
      "Chunk 200: total samples so far = 2945\n",
      "Chunk 201: total samples so far = 2964\n",
      "Chunk 202: total samples so far = 2964\n",
      "Fertig! Samples gespeichert in: C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\politics_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameter\n",
    "input_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\reddit\\subreddits24\\politics_comments.csv\"\n",
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\politics_sampled.csv\"\n",
    "target_subreddit = \"politics\"\n",
    "samples_per_month = 19\n",
    "chunksize = 10**6\n",
    "\n",
    "cols = [\"id\", \"parent_id\", \"link_id\", \"author\", \"body\", \"created_utc\", \"score\", \"subreddit\"]\n",
    "dtypes = {\n",
    "    \"id\": \"string\", \"parent_id\": \"string\", \"link_id\": \"string\",\n",
    "    \"author\": \"string\", \"body\": \"string\", \"created_utc\": \"int64\",\n",
    "    \"score\": \"int32\", \"subreddit\": \"string\"\n",
    "}\n",
    "\n",
    "reservoirs = defaultdict(list)\n",
    "counts = defaultdict(int)\n",
    "start_year, end_year = 2012, 2024\n",
    "\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(input_file, usecols=cols, dtype=dtypes, chunksize=chunksize):\n",
    "    chunk_num += 1\n",
    "    subset = chunk[chunk[\"subreddit\"] == target_subreddit]\n",
    "    created_dt = pd.to_datetime(subset[\"created_utc\"], unit='s')\n",
    "    months = created_dt.dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    for row, month in zip(subset.itertuples(index=False), months):\n",
    "        if pd.isna(row.body) or row.body in (\"[deleted]\", \"[removed]\", \"\"):\n",
    "            continue\n",
    "        year = int(month[:4])\n",
    "        if start_year <= year <= end_year:\n",
    "            counts[month] += 1\n",
    "            idx = counts[month]\n",
    "            if len(reservoirs[month]) < samples_per_month:\n",
    "                reservoirs[month].append(row)\n",
    "            else:\n",
    "                j = random.randint(0, idx - 1)\n",
    "                if j < samples_per_month:\n",
    "                    reservoirs[month][j] = row\n",
    "\n",
    "    if chunk_num % 1 == 0:\n",
    "        total_samples = sum(len(v) for v in reservoirs.values())\n",
    "        print(f\"Chunk {chunk_num}: total samples so far = {total_samples}\")\n",
    "\n",
    "# Flatten reservoirs\n",
    "final_samples = []\n",
    "for month, rows in sorted(reservoirs.items()):\n",
    "    for row in rows:\n",
    "        r = row._asdict()\n",
    "        r[\"month\"] = month\n",
    "        final_samples.append(r)\n",
    "\n",
    "df_samples = pd.DataFrame(final_samples)\n",
    "df_samples.to_csv(output_file, index=False)\n",
    "print(\"Fertig! Samples gespeichert in:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c89d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with smaller sample size \n",
    "Subreddit = \"politics\"\n",
    "output_file = r\"C:\\Users\\TG2023\\OneDrive\\Desktop\\Uni Konstanz\\2. Semester\\Social Media Data Analysis\\Final Project\\politics_with_scores.csv\"\n",
    "samples_politics = pd.read_csv(output_file)\n",
    "# test = samples_politics.sample(n=40, random_state=42) \n",
    "# Generate_Incivility_Ratings(test, Subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Subreddit = \"politics\"\n",
    "Generate_Incivility_Ratings(samples_politics, Subreddit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
